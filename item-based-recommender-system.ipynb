{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run this program\n",
    "First, please make sure that you have **python3** installed (preferably **Anaconda** package).\n",
    "\n",
    "Then use **jupyter notebook** to run the **.ipynb** file.\n",
    "\n",
    "If you have any missing python modules, please install them using **pip install**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "Load the MovieLens data (educational version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data_path = \"ml-latest-small/ratings.csv\"\n",
    "data = pd.read_csv(data_path, sep=',', header=0)\n",
    "data = data[['userId', 'movieId', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_ids = sorted(set(data['userId']))\n",
    "movie_ids = sorted(set(data['movieId']))\n",
    "n_users = len(user_ids)\n",
    "n_movies = len(movie_ids)\n",
    "\n",
    "print(\"Number of users: {}\\nNumber of movies: {}\".format(n_users, n_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show how many users have rated a certain movie\n",
    "vector_sizes = data.groupby('movieId')['userId'].nunique().sort_values(ascending=False)\n",
    "print(vector_sizes)\n",
    "print('On average, each movie is rated {} times'.format(vector_sizes.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show examples of the data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean centering\n",
    "Subtract mean of rating for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# find the mean of each user\n",
    "user_group = data.groupby(by='userId')\n",
    "user_means = user_group['rating'].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a new column named \"meanCenteredRating\"\n",
    "\n",
    "# this function takes in ratings of one user and return mean_centered ratings of that user\n",
    "mean_centering = lambda ratings: ratings - ratings.mean()\n",
    "data['meanCenteredRating'] = user_group['rating'].transform(mean_centering)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show user means\n",
    "user_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data\n",
    "Split the data into training set and test set. Prepare it as a user-item ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomly split the data set\n",
    "test_size = 0.2\n",
    "data_train, data_test = train_test_split(data, test_size=test_size, random_state=42)\n",
    "\n",
    "# show shape of the data\n",
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# build userId to row mapping dictionary\n",
    "user2row = dict()\n",
    "row2user = dict()\n",
    "for i, user_id in enumerate(user_ids):\n",
    "    user2row[user_id] = i\n",
    "    row2user[i] = user_id\n",
    "\n",
    "# build movieId to column mapping dictionary\n",
    "movie2col = dict()\n",
    "col2movie = dict()\n",
    "for i, movie_id in enumerate(movie_ids):\n",
    "    movie2col[movie_id] = i\n",
    "    col2movie[i] = movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# turn ratings data in table format into a user-item rating matrix\n",
    "# the field will be filled with NaN if user didn't provide a rating\n",
    "def data_to_matrix(data):\n",
    "    mat = np.full((n_users, n_movies), np.nan, dtype=np.float32)\n",
    "    for idx, row in data.iterrows():\n",
    "        mat[user2row[row['userId']], movie2col[row['movieId']]] = row['meanCenteredRating']\n",
    "    return mat\n",
    "\n",
    "# prepare the data as a user-item rating matrix for the next step\n",
    "train_ratings = data_to_matrix(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute similarity matrix\n",
    "Build the item-item similarity matrix. This section takes most of the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a blank similarity matrix containing zeros\n",
    "%time sim_matrix = np.zeros((n_movies, n_movies), dtype=np.float32)\n",
    "sim_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove co-elements from 2 vectors if at least one of them is NaN\n",
    "def remove_nans(a, b):\n",
    "    # assuming that a and b are 1-d vectors, create a new axis for both of them\n",
    "    a = a[..., np.newaxis]\n",
    "    b = b[..., np.newaxis]\n",
    "    concat = np.concatenate([a, b], axis=1)\n",
    "    nonan = concat[~np.isnan(concat).any(axis=1)]\n",
    "    return nonan[:, 0], nonan[:, 1]\n",
    "\n",
    "# show examples of how to use remove_nans()\n",
    "a = np.array([-1,2     ,np.nan,4])\n",
    "b = np.array([-2,np.nan,3     ,5])\n",
    "remove_nans(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate a similarity value given 2 vectors\n",
    "# the output is a value between -1 and 1\n",
    "# min_co_elements is the number that determine whether to output NaN\n",
    "# or output the similarity value, if co-elements are too low, the similarity\n",
    "# will not be a good estimate, e.g. if there is only 1 co-element then the output\n",
    "# will only be either -1 or 1, that's sometimes not desirable, so a threshold should be given\n",
    "def calsim(item1, item2, min_co_elements=1):\n",
    "    item1, item2 = remove_nans(item1, item2)\n",
    "    if item1.size == 0 or item1.size < min_co_elements: # item1 and item2 must have the same size at this point\n",
    "        return np.nan\n",
    "#     print(item1.size)\n",
    "    dot = item1.dot(item2)\n",
    "    # find magnitude A.K.A. length of the vector by taking sqrt of the sum of squares of each element\n",
    "    norm1 = np.linalg.norm(item1)\n",
    "    norm2 = np.linalg.norm(item2)\n",
    "    return dot / (norm1 * norm2)\n",
    "\n",
    "# show example of how to use calsim()\n",
    "calsim(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate all the similarities\n",
    "for item1 in range(n_movies):\n",
    "    item1vector = train_ratings[:, item1]\n",
    "    for item2 in range(item1, n_movies):\n",
    "        item2vector = train_ratings[:, item2]\n",
    "        sim = calsim(item1vector, item2vector, min_co_elements=2)\n",
    "        sim_matrix[item1, item2] = sim\n",
    "        sim_matrix[item2, item1] = sim\n",
    "    if (item1+1) % 50 == 0 or item1+1 == n_movies:\n",
    "        print(\"Progress: {}/{} ({:.2f} %) items calculated\".format(item1+1, n_movies, (item1+1)*100/n_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# this sim matrix takes a lot of time to compute,\n",
    "# so saving it to the disk will help saving time in the future\n",
    "np.save('sim_matrix', sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Fractions of similarity matrix that are NaN:', np.isnan(sim_matrix).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation\n",
    "Test recommendation using the item-item similarity matrix built previously.\n",
    "\n",
    "1. We first need to define a predict() function then use it repeatedly to predict rating of every movie of a given user.\n",
    "2. We then sort the predictions and show movies with top predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a predict function which receives row and column in the ratings matrix\n",
    "# then output a rating value (without mean addition), or np.nan if there are no co-items\n",
    "# user_item is a tuple (user_row, movie_column)\n",
    "# sim_threshold is the similarity threshold of each item,\n",
    "# if the item exceeds this value, it will be chosen for averaging the outcome\n",
    "def predict(ratings, user_item, sim_threshold, debug=True):\n",
    "    desired_user, desired_item = user_item\n",
    "    rating_sum = 0.\n",
    "    total_sim = 0.\n",
    "    for item in range(ratings.shape[1]):\n",
    "        s = sim_matrix[item, desired_item]\n",
    "        rating = ratings[desired_user, item]\n",
    "        if np.isnan(s) or s < sim_threshold or item == desired_item or np.isnan(rating):\n",
    "            continue\n",
    "        rating_sum += s * rating\n",
    "        total_sim += s\n",
    "        if debug:\n",
    "            print('sim and rating of item {}:'.format(item), s, rating)\n",
    "    return rating_sum / total_sim if total_sim else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the similarity threshold value, as the only hyperparameter available\n",
    "sim_threshold = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(train_ratings, (0, 30), sim_threshold), train_ratings[0, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the movie names\n",
    "movie_file = \"ml-latest-small/movies.csv\"\n",
    "movie_df = pd.read_csv(movie_file, header=0)\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# desired_user is the user row that we want to recommend\n",
    "# return recommended item indices sorted by rating descendingly, and the associated score\n",
    "def recommend(ratings, desired_user, sim_threshold):\n",
    "    scores = []\n",
    "    for item in range(ratings.shape[1]):\n",
    "        score = ratings[desired_user, item]\n",
    "        if np.isnan(score):\n",
    "            score = predict(ratings, (desired_user, item), sim_threshold, debug=False)\n",
    "        else:\n",
    "            score = -np.infty # we don't want to recommend movies that user have rated\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    scores_argsort = np.argsort(scores)[::-1]\n",
    "    scores_sort = np.sort(scores)[::-1]\n",
    "    \n",
    "    # numpy will put nan into the back of the array after sort\n",
    "    # when we reverse the array, nan will be at the front\n",
    "    # we want to move nan into the back again\n",
    "    # so we use a numpy trick which rolls the array value\n",
    "    # source: https://stackoverflow.com/a/35038821/2593810\n",
    "    no_of_nan = np.count_nonzero(np.isnan(scores))\n",
    "    scores_argsort = np.roll(scores_argsort, -no_of_nan)\n",
    "    scores_sort = np.roll(scores_sort, -no_of_nan)\n",
    "    return scores_argsort, scores_sort\n",
    "\n",
    "def recommend_msg(user_row, scores_argsort, scores_sort, how_many=10):\n",
    "    m = user_means.loc[row2user[user_row]]['mean']\n",
    "    print('User mean rating:', m)\n",
    "    msg = pd.DataFrame(columns=['movieId', 'title', 'genres', 'rating'])\n",
    "    for i in range(how_many):\n",
    "        col = scores_argsort[i]\n",
    "        movie_id = col2movie[col]\n",
    "        movie = movie_df.loc[movie_df['movieId'] == movie_id].iloc[0]\n",
    "        msg.loc[i+1] = [movie_id, movie['title'], movie['genres'], scores_sort[i] + m]\n",
    "    msg['movieId'] = msg['movieId'].astype(np.int32)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "user = 0 # the given user\n",
    "scores_argsort, scores_sort = recommend(train_ratings, user, sim_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_argsort, scores_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommend_msg(user, scores_argsort, scores_sort, how_many=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Evaluate the error on the test set. The error metric chosen in our work is **MAE**.\n",
    "1. We need to predict mean centered ratings of every (user,movie) pair in the test data\n",
    "2. Take the difference between the true ratings and the predicted ratings\n",
    "3. Take the absolute\n",
    "4. Take the mean\n",
    "\n",
    "And that's how the error is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first, let's take a look at some of the test data\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict ratings for the given data table\n",
    "def predict_table(data_test, sim_threshold, show_progress=True):\n",
    "    n_test = data_test.shape[0]\n",
    "    predictions = np.empty((n_test,))\n",
    "    i = 0\n",
    "    for idx, row in data_test.iterrows():\n",
    "        pred = predict(train_ratings, (user2row[row['userId']], movie2col[row['movieId']]), sim_threshold, debug=False)\n",
    "        predictions[i] = pred\n",
    "        if show_progress and ((i+1) % 100 == 0 or i+1 == n_test):\n",
    "            print(\"Progress: {}/{} ({:.2f} %) ratings predicted\".format(i+1, n_test, (i+1)*100/n_test))\n",
    "        i += 1\n",
    "    if show_progress:\n",
    "        print(\"Progress: {}/{} ({:.2f} %) ratings predicted\".format(i+1, n_test, (i+1)*100/n_test))\n",
    "    return predictions\n",
    "\n",
    "def eval_error(data_test, predictions):\n",
    "    return np.abs(data_test['meanCenteredRating'] - predictions).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# predicting ratings for every (user,movie) pair in the test data\n",
    "predictions = predict_table(data_test, sim_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test['prediction'] = predictions\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test['abs_error'] = np.abs(data_test['meanCenteredRating'] - data_test['prediction'])\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mean absolute error\n",
    "mae = data_test['abs_error'].mean()\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Optimization\n",
    "Find the best set of hyperparameters that yields the lowest error on the test set.\n",
    "In this work, we use **sim_threshold (similarity threshold)** as the only hyperparameter of the system.\n",
    "\n",
    "We can find the best **sim_threshold** by iteratively\n",
    "1. varying its value\n",
    "2. predict outcome on the test set\n",
    "3. evaluate the error\n",
    "4. if the error is less than the least error found so far, save current **sim_threshold** as the best candidate\n",
    "\n",
    "Repeat this cycle until enough satisfaction is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a set of avaialble similarity thresholds\n",
    "candidate_sim_thresholds = np.linspace(-1, 0.75, num=8)\n",
    "candidate_sim_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "errors = np.empty_like(candidate_sim_thresholds, dtype=np.float32)\n",
    "for i, sim_threshold in enumerate(candidate_sim_thresholds):\n",
    "    print('Current similarity threshold:', sim_threshold)\n",
    "    predictions = predict_table(data_test, sim_threshold, show_progress=False)\n",
    "    error = eval_error(data_test, predictions)\n",
    "    print('Error:', error)\n",
    "    errors[i] = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_error_idx = np.argmin(errors)\n",
    "best_error = errors[best_error_idx]\n",
    "best_sim_threshold = candidate_sim_thresholds[best_error_idx]\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Optimal similarity threshold:', best_sim_threshold)\n",
    "print('Optimal error:', best_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the error as a function of **sim_threshold**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(candidate_sim_thresholds, errors, 'r*--')\n",
    "plt.xlabel('similarity threshold')\n",
    "plt.ylabel('MAE (mean absolute error)')\n",
    "plt.grid()\n",
    "plt.title('error as a function of sim_threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Inference on the Real World\n",
    "This is the last step, all we have done to this point is now on production.\n",
    "\n",
    "**Our task:** Given a user, recommend some movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose a user_id from the data\n",
    "user_id = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "user_row = user2row[user_id]\n",
    "scores_argsort, scores_sort = recommend(train_ratings, user_row, best_sim_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommend_msg(user_row, scores_argsort, scores_sort, how_many=10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
